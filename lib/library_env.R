#--------------------------------------------------------------------------#
# Project: VISIIS
# Script purpose: Regroup functions used for environemental data processing of visufront
# Date: 02/12/2020
# Author: Thelma Panaiotis
#--------------------------------------------------------------------------#



## Angle unit conversion ----
#--------------------------------------------------------------------------#
rad2deg <- function(rad) {(rad * 180) / (pi)}
deg2rad <- function(deg) {(deg * pi) / (180)}


## Read ISIIS env data file without renaming columns ----
#--------------------------------------------------------------------------#
read_isiis <- function(file) {
  #' Read and ISIIS env data file
  #'
  #' Returns the content of ISIIS env data file, including datetime, with unchanged column names and with supplementary columns datetime and date
  #' @param file Name of the ISIIS file

  library(stringr)
  
  # read the data
  d <- read.delim(file, sep="\t", skip=10, fileEncoding="ISO-8859-1", check.names=FALSE)
  
  # add date, including day change detection
  d <- d %>% 
    mutate(
      date = as_date(str_c(str_sub(file, -16, -13), "-", str_sub(file, -12, -11), "-", str_sub(file, -10, -9), " ")), # compute date
      datetime = ymd_hms(str_c(date, " ", Time), tz="Europe/Paris"), # compute datetime with local time zone
      diff_time = datetime - lag(datetime) # compute difference between datetime and lag(datetime)
      )
  
  if (any(d$diff_time < 0, na.rm = T)) { # if any of these values is negative, there is a day change
    d$rownumber = 1:dim(d)[1] # compute row number
    d <- d %>% 
      mutate(
        # for all rows below negative diff_time, add +1 day to datetime
        datetime = ifelse(rownumber >= which(diff_time < 0), datetime + days(1), datetime) %>% as_datetime(), 
        date = date(datetime)
        ) %>% 
      select(-rownumber)
  }
  
  # drop temporary column
  d <- d %>% select(-diff_time)
  
  return(d)
}

## Read thermosalinometer file from Tethys ----
#--------------------------------------------------------------------------#
read_ts <- function(file) {
  #' Read a TS file generated by Tethys, keep only lat, lon and datetime
  #'
  #' Returns the lat, lon and datetime from the Tethys file
  #' @param file Name of the TS file
  
  library(lubridate)
  
  # Get column names from "notice_daufin-TS.pdf" page 12
  colNames <- c(
    "errorCode", "timeUTC", "lat", "lon", "PDOP", "nbSatellites", "sep",
    "timeUTC_human", "latDegMin", "latNS", "lonDegMin", "lonEW", "gpsQuality", "nbSatellites", "HDOP", "sep",
    "atmPressure", "tempAir", "humidity", "windDirection", "windSpeed", "sep",
    "bearing", "shipSpeed", "sep",
    "bearingTrue", "shipSpeedTrue", "sep",
    "depth", "sep",
    "tempCell", "conductivity", "temperature", "salinity", "fluorometry",
    "empty", "empty", "empty", "empty"
  )
  
  # Read data
  df <- read.table(file, sep="\t", header=FALSE, col.names=colNames) %>% 
    as_tibble() %>% 
    # compute date_time
    mutate(
      date = ymd(str_c(str_sub(file, -15, -12), "-", str_sub(file, -11, -10), "-", str_sub(file, -9, -8))),
      time = as_datetime(timeUTC),
      datetime_utc = time + days(date(date)),
      datetime = force_tz(datetime_utc, tz="Europe/Paris")
      ) %>% 
    select(-c(date, time))
  
  # fill the gaps in GPS coordinates when necessary
  if (any(is.na(df$lat))) {
    df$lat <- approx(x=df$datetime_utc, y=df$lat, xo=df$datetime_utc)$y
    df$lon <- approx(x=df$datetime_utc, y=df$lon, xo=df$datetime_utc)$y
  }
  
  # select interesting data columns
  df <- df %>% select(lat, lon, datetime_utc, datetime)
  
  return(df)
}

## Rename ISIIS files columns ----
#--------------------------------------------------------------------------#
rename_isiis_cols <- function(df) {
  #' Rename columns of ISIIS data and convert to tibble.
  #'
  #' @param df Dataframe containing ISIIS data
  #' 

  # Keep relevant columns and give appropriate names
  df <- df %>% 
    select(
      datetime,
      lon      = `Long (decimals)`,
      lat      = `Lat (decimals)`,
      press    = `Pressure (dbar)`,
      depth    = `Depth (m)`,
      temp     = `Temp (°C)`,
      sal      = `Salinity (PPT)`,
      fluo     = `Fluoro (volts)`,
      oxy      = `Oxygen (ml/l)`,
      irrad    = `Irrandiance (UE/cm²)`,
      hor_vel  = `Horizontal vel in water (m/s)`,
      vert_vel = `Vertical vel (m/s)`,
      pitch    = `Pitch (°)`,
      roll     = `Roll (°)`,
      heading  = `Heading (°)`,
    ) %>% 
    as_tibble() # convert to tibble
  
  return(df)
}


## Compute yo numbers and types ----
#--------------------------------------------------------------------------#
compute_cast <- function(d, order = 200, n = 5){
  #' Rename columns of ISIIS data and convert to tibble.
  #'
  #' @param d Dataframe containing ISIIS data to compute casts on
  #' @param order order of window for depth smoothing (weighted moving average)
  #' @param n number of times to smooth the depth

  library(castr)
  library(pastecs)
  
  # Compute smoothed depth
  d <- d %>% mutate(depth_sm = smooth(depth, k = order, n = n))
  
  # Compute turning points on smoothed depth
  TP <- suppressWarnings(turnpoints(d$depth_sm))
  # Compute cast numbers
  yo <- cumsum(TP$peaks | TP$pits) + 1
  
  # Detect which are up and which are down casts: depths are positive values, so if the 1st turning point is a peak, it is in reality a pit, so the first cast is downcast
  if (TP$firstispeak) {
    cast_types <- c("up", "down")
  } else {
    cast_types <- c("down", "up")
  }
  yo_type <- cast_types[yo %% 2 + 1]
  
  # Add cast number to df and discard smoothed depth
  d <- d %>% 
    mutate(
      yo = factor(yo),
      yo_type = yo_type,
    ) %>% 
    select(-depth_sm)
  
  return(d)
}

## Compute yo numbers and types with Python ----
#--------------------------------------------------------------------------#
compute_cast_python <- function(d, k = 10, n = 5, width = 200, distance = 200){
  #' Rename columns of ISIIS data and convert to tibble.
  #'
  #' @param d Dataframe containing ISIIS data to compute casts on
  #' @param k order of window for depth smoothing (weighted moving average)
  #' @param n number of times to smooth the depth
  #' @param width Required width of peaks in samples
  #' @param distance required minimal horizontal distance (>= 1) in samples between neighbouring peaks 
  
  library(reticulate)
  library(castr)
  scipy <- import("scipy")
  
  # Compute smoothed depth
  depths <- d %>% pull(depth) %>% smooth(k = k, n = n)
  
  # Find peaks and pits indexes
  peaks_idx = scipy$signal$find_peaks(depths, width = width, distance = distance)[[1]]
  pits_idx = scipy$signal$find_peaks(depths*(-1), width = width, distance = distance)[[1]]
  
  # Initialize all peaks and pits to FALSE
  peaks <-  rep_len(FALSE, length.out = length(depths))
  pits <-  rep_len(FALSE, length.out = length(depths))
  
  # When a peak or pit index is reached, set it as TRUE
  peaks[peaks_idx] <- TRUE
  pits[pits_idx] <- TRUE
  
  # Compute yo numbers
  yo <- cumsum(peaks | pits) + 1
  
  # Compute yo type: depths are positive values, if first TP is a peak, the first cast is down
  if (peaks_idx[1] < pits_idx[1]){
    cast_types <- c("up", "down")
  } else {
    cast_types <- c("down", "up")
  }
  yo_type <- cast_types[yo %% 2 + 1]
  
  # Add cast number to df and discard smoothed depth
  d <- d %>% 
    mutate(
      yo = factor(yo),
      yo_type = yo_type,
    ) 
  
  return(d)
}


## Round a variable to chosen precision ----
#--------------------------------------------------------------------------#
round_any <- function (x, accuracy, f=round){f(x/accuracy) * accuracy}


## Interpolate missing values with dist and depth ----
#--------------------------------------------------------------------------#
interp_missing = function(df, my_var){
  #' Interpolate missing values at points a transect.
  #' 
  #' Interpolate discrete missing values using a bivariate interpolation
  #' (akima) followed by a nearest neighbour interpolation (scipy.interpolate)
  #' for points outside of the convex hull using dist and depth values. 
  #' @param df Dataframe containing ISIIS data to interpolate for one transect
  #' @param my_var name of variable to interpolate
  
  library(akima)
  library(reticulate)
  int <- import("scipy.interpolate", as="int")
  
  # Extract relevant columns 
  df <- df %>% select(dist, depth, all_of(my_var))
  
  # Remove missing data
  df_wo_na <- df %>% drop_na(all_of(my_var))
  
  # Bivariate interpolation with akima
  options(warn=-1) # disable warnings to hide akima::interpp warning
  int_1 = akima::interpp(
    x = df_wo_na$dist,
    y = df_wo_na$depth, 
    z = df_wo_na[my_var] %>% pull(),
    xo = df$dist, 
    yo = df$depth,
    extrap = TRUE, duplicate = "mean", linear = T
  )
  options(warn=0) # turn on warnings
  df$value <- int_1$z # store result
  
  # Nearest neighbour interpolation with scipy.interpolate
  int_2 <- int$griddata(
    points=as.matrix(df %>% drop_na(value))[,1:2], 
    values=df %>% drop_na(value) %>% pull(value), 
    xi=as.matrix(df)[,1:2], 
    method="nearest"
  )
  df$value <- int_2 # store result
  
  # Reformat output
  df_int <- df %>% 
    mutate(
      variable = my_var,
      transect = my_transect
    ) %>% 
    spread(variable, value)
  
  return(df_int)
  
}


## Interpolate a variable across a transect ----
#--------------------------------------------------------------------------#
interp_var = function(df, my_var, my_transect, step_dist, step_depth) {
  #' Interpolate a variable across a transect.
  #' 
  #' Compute the linear interpolation of a variable on a given transect 
  #' onto a specified output grid. 
  #' @param df Dataframe containing ISIIS data to interpolate
  #' @param my_var name of variable to interpolate
  #' @param my_transect name of transect to compute interpolation across
  #' @param step_dist step for grid interpolation on x axis (dist)
  #' @param step_depth step for grid interpolation on y axis (depth)
  
  library(akima)
  library(broom)
  library(tidyverse)
  
  # Filter data for this transect only
  df <- df %>% filter(transect == my_transect) 
  
  # Sequence of out dist: from depth min to depth max by step_dist
  xo <- seq(
    from = round(min(df$dist), digits = 1), 
    to   = round(max(df$dist), digits = 1), 
    by   = step_dist
  )
  
  # Sequence of out depth: from depth min to depth max by step_depth
  yo <- seq(
    from = round(min(df$depth)), 
    to = round(max(df$depth)), 
    by = step_depth
  )
  
  # Keep relevant columns and drop NA values
  df <- df %>% 
    select(dist, depth, all_of(my_var)) %>% 
    drop_na(all_of(my_var))
  
  # Perform the interpolation 
  df_int <- interp(
    x=df$dist, 
    y=df$depth, 
    z=df[my_var] %>% pull(), 
    linear=TRUE, 
    xo = xo,
    yo = yo,
  ) %>% 
    tidy() %>% # convert list to dataframe
    select(dist = x, depth = y, value = z) %>% # rename columns
    # change name of column containing newly interpolated variable with mutate and spread
    mutate(
      variable = my_var,
      transect = my_transect,
    ) %>% 
    spread(variable, value)
  
  return(df_int)
}


## Compute anomaly to the mean ----
#--------------------------------------------------------------------------#
compute_anom <- function(x){return (x - mean(x, na.rm = TRUE))}


## Count missing data ----
#--------------------------------------------------------------------------#
# Proportion of NA in a vector
na_prop <- function (x){sum(is.na(x))/length(x)}

# Sum of NA in a vector
na_sum <- function (x){sum(is.na(x))}
